---
layout: post
title: "[DeepLearning]深度学习之一基础概念"
description: "本文介绍了深度学习相关的一些基础知识和基本概念。"
category: 'project experience'
---

机器学习 = 表示+评估+优化

表示是指由输入如何得到输出，评估是指估计输出或者输入的分布，优化是用来逼近分布。

机器学习是为了拟合真实分布，从而得到未知分布。针对解决两类问题，一种是分类问题classification，给了输入，输出独立且得到确定分类。另一种是回归问题regression，针对给的输入，通过训练出来的模型能够预测出输出值，这个输出值是连续分布的。


####1.学习分类

【监督学习】：训练有label（输出已知）的例子，例如手写体识别，目的是根据已知的映射关系去推断未知，能够很快的给出新的点label，计算量小。

【无监督学习】：训练无label的例子，不产生输入到输出的映射，但是也有目标函数，也是对目标函数进行优化。

【半监督学习】：以上两者结合

【transduction】：观察到的特定的训练集来预测特定的固定的测试集，关键在于推理，不是归纳模型，通过一些测试集相互矛盾的推测得到推论（TSVM），用来求近似值较好，而且test集可以是任意分布，而半监督则是与训练集相关。好处是需要较少的labeled点来预测分类，并且能够考虑到所有的点，自己标记那些unlabeled点通过他们属于哪个集群。缺点是不会建立预测模型，假如有新的点插入那么就需要重新把所有点都遍历一遍，数据大的时候计算损耗大。

【强化学习】：与行为主义心理学有关，环境是马尔可夫决策过程(MDP)，与动态编程技术有关，不需要知道具体的MDP因为需要解决非常大的MDP。区别是没有正确的输入输出对，也不会纠正求出错误的label的行为，关注点在持续的性能表现，在未被探索和已开发的领域找到一个平衡点。例如赌博机中，要得到最大的输出值，而不介意每次的组合是什么，赌徒要尽快找到获得最大奖励的手臂。就是有很多种组合，只找最优组合

【多任务学习（mtl）】：并行学习多个相关的problems（？），多个label？例如邮件过滤，个体垃圾邮件不同，但是肯定有相同的，例如可以使全部的权重值变小一点。用来预测没有给定输入的输出，通过之前的训练经验，训练出一个确定的输出（例子），就算在训练集中没有得出，但是也能推测出。inductive bias对目标概念作一些假设（前提）。

####2.概念

【泛化（generalization）】就是将测试集和训练集分开，过拟合（overfitting）得到的分布接近训练集，但是不逼近测试集，这个时候，目标函数会顾及每一个点，导致形成的拟合函数波动很大，每个地方的导数很大，这样虽然把所有点都包括进去，但是函数变化很复杂，只有系数足够打，才能保证导数值大。交叉验证可以帮助避免过拟合（就是用验证集validation）。一般的避免方式是加入正则项（regularization term），正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。

【奥卡姆剃刀原理】：这个原理称为“如无必要，勿增实体”，就是越简单越好

【归纳偏置】：搜索路径则是机器学习算法的核心问题，找到最优的路径，用梯度下降的方法，搜索范围则定义为归纳偏置，有了它，当随着权重一步步更改的时候，它也在一步步逼近平衡点

【特征】：其实特征是一点一点构成的，小的特征组合形成大的特征，上一层看下一层是pixel级别的，下一层称为上一层的basis，每一层都是输入的另一种表示，不能出现信息的损失。

参考：

[http://mlg.eng.cam.ac.uk/zoubin/nipstut.pdf](http://mlg.eng.cam.ac.uk/zoubin/nipstut.pdf)

[http://www.valleytalk.org/wp-content/uploads/2012/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%82%A3%E4%BA%9B%E4%BA%8B.pdf](http://www.valleytalk.org/wp-content/uploads/2012/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%82%A3%E4%BA%9B%E4%BA%8B.pdf)















































